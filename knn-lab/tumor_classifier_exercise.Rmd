---
title: "Tumor Classifier - K-Nearest Neighbours"
author: "Your name"
date: "September 11, 2024"
output: 
  html_document:
    toc: true
    number_sections: true

---

# Tumour Classifier with K-Nearest Neighbours (KNN)

In this exercise, you'll build a KNN classifier to distinguish between tumor classes based on a subset of features. Complete the steps by filling in the missing code and answering questions.

You will use the `caret` package for this exercise. Use the 'help' in RStudio to discover how they work. The key functions that you will use are:

- createDataPartition
- trainControl
- train
- predict
- confusionMatrix

## Instructions

### Step 1: Install and load necessary libraries

Fill in the missing code to install and load the following libraries: tidyverse, here, and caret.

```{r }
# Install packages if you haven't already
install.packages("caret")

library(tidyverse) 
library(here)
library(caret)
```

### Step 2: Load the dataset

```{r load-data}
cancer_data <- read_csv(here("data", "synthetic_cancer_data.csv"))
cancer_data
```

### Step 3: Prepare the data

-   Select the three relevant variables; Perimeter, Concavity, Class
-   Convert the target variable to a factor
-   Split the data into 80% training and 20% testing sets

```{r}

# Keep only the three relevant columns
data_subset <- cancer_data %>% 
  select(Perimeter, Concavity, Class)

# Encode the target variable as a factor
data_subset$Class <- as.factor(data_subset$Class)

data_subset <- data_subset %>% 
  mutate(Class = as.factor(Class))
  
# Set a seed for reproducibility
set.seed(42)

# Split the data into training (80%) and testing (20%) sets
train_index <- createDataPartition(data_subset$Class, p = 0.8, list = FALSE)

training_dataset <- data_subset %>% 
  slice(train_index)

testing_dataset <- data_subset %>% 
  slice(-train_index)

```

*Question*: Why is it important to set a seed before splitting the data?

*Answer*: for reproduceability and repeatability

### Step 4: Train the KNN model

-   Define a cross-validation control.
-   Train the KNN model, tuning the number of neighbours 'k' using tuneLength = 10.

```{r}
# Define training control (using 10-fold cross-validation)
trainControl <- trainControl(number = 10, method = "cv" )

# Train the KNN model 
knn_model <- train(Class ~ ., 
                 data = training_dataset, 
                 preProcess = c("scale", "center"),
                 method = "knn",
                 trControl = trainControl,
                 tuneLength = 10)

# View the results
knn_model

```

*Question*: What is the purpose of using cross-validation when training the model?

*Answer*: train the model many times for better accuracy

### Step 5: Make predictions

-   Use the trained model to predict on the test data.
-   Assess the performance using a confusion matrix.

```{r}
# Make predictions on the test set
predictions <- predict(knn_model, newdata = testing_dataset)

predictions

# Confusion matrix  
confusionMatrix(predictions, testing_dataset$Class)

```

*Question*: What insights can you gather from the confusion matrix? Is the model performing well?

*Answer*: ...

### Step 6: Tune the model (optional)

-   Explore the best value of 'k' from the trained model.
-   Discuss how the number of neighbors (k) impacts model performance.

```{r}
# Check the best value of 'k'
best_kvalue<- knn_model$bestTune
print(best_kvalue)
```

*Question*: What is the optimal value of 'k'? How would you explain its significance?

*Answer*: 5
